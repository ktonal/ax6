{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9301121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mimikit as mmk\n",
    "import h5mapper as h5m\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.wavenets import WaveNetFFT, WaveNetQx\n",
    "from models.srnns import SampleRNN\n",
    "from models.s2s import Seq2SeqLSTM\n",
    "from models.ensemble import Ensemble\n",
    "from mains import train, generate\n",
    "\n",
    "from checkpoints import group_ckpts_by_trainset, load_feature, load_files, load_network_cls\n",
    "from datasets import TRAINSET, VERDI_X\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mimikit.extract.from_neighbors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"trainings/wn-verdi-x\"\n",
    "\n",
    "CKPTS = group_ckpts_by_trainset(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e889a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc01cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2730*16*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7312d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(64).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359bfc2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ckpt in CKPTS.values():\n",
    "    net_cls, ckpt, feat, epochs, hp = ckpt[0]\n",
    "    \n",
    "    print(feat)\n",
    "    train = load_files(hp[\"files\"], feat.sr)\n",
    "    y = feat.transform(train.snd[:])\n",
    "    \n",
    "    for output in h5m.FileWalker(h5m.Sound.__re__, root+\"/\"+hp[\"id\"]):\n",
    "        x = h5m.Sound(sr=feat.sr).load(output)\n",
    "        print(output)\n",
    "        mmk.audio(x, sr=feat.sr)\n",
    "        x = feat.transform(x)\n",
    "        \n",
    "        X = torch.as_tensor(x).unsqueeze(0).to(\"cuda\")\n",
    "        Y = torch.as_tensor(y).unsqueeze(0).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, nn = nearest_neighbor(X, Y)\n",
    "            rr = repeat_rate(nn, 88, 1)\n",
    "            items, idx = torch.unique(nn, return_inverse=True)\n",
    "            cum_probs = torch.zeros(nn.size(0), items.size(0), nn.size(1))\n",
    "            cum_probs[:, idx, torch.arange(nn.size(1))] = 1\n",
    "            cum_probs = torch.cumsum(cum_probs, dim=2)\n",
    "            print(cum_probs)\n",
    "            \n",
    "            cum_probs = cum_probs / cum_probs.sum(dim=1, keepdims=True)\n",
    "            e_wrt_t = (-cum_probs*torch.where(cum_probs > 0, torch.log(cum_probs), cum_probs)).sum(dim=1)\n",
    "            print((torch.sign(e_wrt_t[:, 1:] - e_wrt_t[:, :-1]) * e_wrt_t[:, :-1]).sum(dim=1))\n",
    "            \n",
    "        plt.figure(figsize=(18, 4))\n",
    "        plt.plot(nn.cpu().numpy()[0])\n",
    "        plt.figure(figsize=(18, 4))\n",
    "        plt.hist(nn.cpu().numpy()[0], bins=512)\n",
    "        plt.figure(figsize=(18, 4))\n",
    "        plt.plot(rr.cpu().numpy()[0] * e_wrt_t.cpu().numpy()[0].max().item())\n",
    "        plt.plot(e_wrt_t.cpu().numpy()[0])\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d34e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4036e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950428e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bf5c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CKPTS = group_ckpts_by_trainset(\"trainings\")\n",
    "[*CKPTS.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d23efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = [*CKPTS.keys()][7]\n",
    "CKPTS[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fff4132",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5mapper as h5m\n",
    "import mimikit as mmk\n",
    "from pbind import *\n",
    "\n",
    "from models.ensemble import Ensemble\n",
    "from datasets import COUGH\n",
    "from checkpoints import load_files\n",
    "\n",
    "stream = Pseq([\n",
    "    Pbind(\n",
    "        \"id\", \"80cb7d5b4ff7af169e74b3617c43580a41d5de5bd6c25e3251db2d11213755cd\",\n",
    "        \"epoch\", Prand([40, 50], inf),\n",
    "        \"seconds\", Pwhite(lo=1., hi=8., repeats=1)\n",
    "        ),\n",
    "    Pbind(\n",
    "        \"id\", \"80cb7d5b4ff7af169e74b3617c43580a41d5de5bd6c25e3251db2d11213755cd\",\n",
    "        \"epoch\", Prand([40, 50], inf),\n",
    "        \"seconds\", Pwhite(lo=0.5, hi=1.5, repeats=1)\n",
    "        ),\n",
    "], inf).asStream()\n",
    "    \n",
    "ensemble = Ensemble(60., 22050, stream)\n",
    "\n",
    "def process_outputs(outputs, bidx):\n",
    "    mmk.audio(outputs[0][0].cpu().numpy(), sr=ensemble.base_sr)\n",
    "\n",
    "prompt_files = load_files(COUGH[\"Cough\"], ensemble.base_sr)\n",
    "prompt = prompt_files.snd[0:44100]\n",
    "prompt = torch.as_tensor(prompt).unsqueeze(0)\n",
    "\n",
    "loop = mmk.GenerateLoop(\n",
    "    network=ensemble,\n",
    "    dataloader=[(prompt,)],\n",
    "    inputs=(h5m.Input(None, \n",
    "                      getter=h5m.AsSlice(dim=1, shift=-ensemble.base_sr, length=ensemble.base_sr),\n",
    "                      setter=h5m.Setter(dim=1)),),\n",
    "    n_steps=int(ensemble.base_sr * ensemble.max_seconds),\n",
    "    add_blank=True,\n",
    "    process_outputs=process_outputs\n",
    ")\n",
    "loop.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef9a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = mmk.Spectrogram(sr=44100, normalize=True, emphasis=0.0, n_fft=1024, hop_length=256, coordinate='mag', center=True)\n",
    "feat.inverse_transform_(torch.randn(1, 20, 513)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.max_seconds * ensemble.base_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed95656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08894ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa8544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d515bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aa6a601",
   "metadata": {},
   "source": [
    "# Stream Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac43b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5mapper as h5m\n",
    "\n",
    "from checkpoints import group_ckpts_by_trainset\n",
    "\n",
    "group_ckpts_by_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625ed16",
   "metadata": {},
   "source": [
    "# Split Checkpoint Banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc860d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5mapper as h5m\n",
    "from google.cloud import storage\n",
    "\n",
    "from checkpoints import CkptBank, load_trainings_hp, load_network_cls, Checkpoint\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def split_by_src(ckpt_path):\n",
    "    print(ckpt_path)\n",
    "    bank = CkptBank(ckpt_path)\n",
    "    hp = bank.ckpt.load_hp()\n",
    "    to_upload = []\n",
    "    \n",
    "    dirname = os.path.dirname(ckpt_path)\n",
    "    train_hp = load_trainings_hp(dirname)\n",
    "    net_cls = load_network_cls(train_hp[\"network_class\"])\n",
    "    hp[\"cls\"] = net_cls\n",
    "    for ep_id in bank.index.keys():\n",
    "        new_path = os.path.join(\n",
    "            dirname, ep_id.split(\"-\")[0] + \".h5\"\n",
    "        )\n",
    "        if os.path.isfile(new_path):\n",
    "            to_upload += [new_path]\n",
    "            continue\n",
    "        new = CkptBank(new_path, mode=\"w\")\n",
    "        new.ckpt.save_hp(hp)\n",
    "        new.flush()\n",
    "        new.ckpt.add(\"state_dict\", new.ckpt.format(bank.get(ep_id)['ckpt']))\n",
    "        new.flush()\n",
    "        new.close()\n",
    "        to_upload += [new_path]\n",
    "    return to_upload\n",
    "\n",
    "def upload_to_gcp(ckpt_path):\n",
    "#     raise ValueError\n",
    "    ck = Checkpoint(*Checkpoint.get_id_and_epoch(ckpt_path))\n",
    "    if not ck.blob.exists():\n",
    "        print(\"uploading\", ck)\n",
    "        ck.blob.upload_from_filename(ckpt_path, timeout=None)\n",
    "    print(ck.blob, ck.blob.exists())\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# to_split = h5m.FileWalker(r\"checkpoints\\.h5\", \"/home/antoine/ktonal/ax6/trainings/s2s-grid-cough\")\n",
    "to_upload = h5m.FileWalker(r\"epoch=.*\\.h5\", \"/home/antoine/ktonal/ax6/trainings/s2s-grid-lungs\")\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=4)\n",
    "# to_upload = [path for x in to_split for path in split_by_src(x)]\n",
    "\n",
    "as_completed([*executor.map(upload_to_gcp, to_upload)])\n",
    "executor.shutdown()\n",
    "1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbf69a2",
   "metadata": {},
   "source": [
    "# Download Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6700a58b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "\n",
    "Trainset.root_dir = \"./train-data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51379730",
   "metadata": {},
   "outputs": [],
   "source": [
    "cough = Trainset(\"Cough\")\n",
    "lungs = Trainset(\"Lung Collection\")\n",
    "cough = cough.download()\n",
    "\n",
    "lungs = lungs.download()\n",
    "cough.index, lungs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76592396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "librosa.load(\"./train-data/Lung Collection/Breath.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610101f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494e23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b43af8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
